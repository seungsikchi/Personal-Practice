Classification은 이미지를 입력받아 이미지 하나에 대한 카테고리를 출력함 
Localization, Detection은 이미지의 레이블을 예측하면서 그 물체가 어디에 있는지 정보를 제공. 물체가 있는곳에 네모를 그리거나 하는방법
Segmentation은 이미지를 입력 받아 이미지의 픽셀을 구분해 픽셀에 대한 정보를 출력함

ResNet
네트워크가 점점 깊어질수록 Vanishing Gradient, Exploding Gradient와 같은 문제 발생 => 이를 해결하기위해서 residual(잔차)을 학습을하면 할수록 점점 더 좋은 결과를 나타내는것이 보임
degradation 문제는 Overfitting이 원인이 아니라, Model의 depth가 깊어짐에 따라 training-error가 높아짐 => 깊이가 깊어지면서 점점 학습시간 증가 => 깊이는 줄이되 학습률이 높음
RseNet 구조
단순한 CNN구조는 x를 받아 2개의 weighted layer를 거쳐 출력 H(x)를 내며 다음 layer의 입력으로 적용됨 그러나 ResNet은 layer의 입력을 layer의 출력에 바로 연결시키는 "skip connection"을 사용했다.

(BN을하고 ReLU로 Convolution시키고 Maxpooling을 함)= 한사이클을 나온결과를 마지막에 출력값에 입력값을 더함 

VGGNet16
3 x 3 conv filter를 고정해 기존 CNN모델의 layer의 갯수를 늘렸고 이것이 large-scale image recognition에서도 좋은 결과를 얻게 만들었다.
레이어 개수는 16개
원래의 con filter size는 kernel size(conv filter)를 다르게 적용을 하지만 VGGNet에는 동일하게 3x3으로 적용하고 있다. => 고의적으로 깊게 만듬
=> 연산하여 발생하는 파라미터값을 줄이는 효과와 ReLU가 활성화 함수로 들어갈 수 있는 곳이 많아진다는 장점이 있음 => 7x7 레이어에서 3x3의 필터를 3번적용하면 5x5의 필터와 동일한 효과를 볼 수 있을 뿐더러 파라미터 개수도 더 적은 27개가 생기게 됨 => 정규화를 할때 이점
활성화 함수는 ReLU를 사용 why? => ReLU는 비선형성을 가지게 해서 CNN에서 레이어를 쌓는다는 의미를 가지게함 
VGGNet19
레이어 개수 19개
레이어가 더 많은 만큼 메모리등의 자원을 많이 소모 하지만 VGG16과 비슷하거나 성능이 떨어지기때문에 잘 사용하지 않음
가장 많은 파라미터의 수를 가지고 있음 기본적은 틀은 VGG16을 따라가고 거기서 레이어만 3개 더 추가됨
추가된 레이어가 Exploding Gradient / vanishing 문제로 인한 깊은 레 학습때 발생하는 문제를 해결함

LeNet-5
Convolution과 Subsampling을 반복적으로 거치면서 마지막에 Fully-connected Multi-layered Neural Network로 classification을 수행함
C1에서는 5x5 Convolution 연산하여 28x28 사이즈의 6개의 feature map을 생성함
s2에서는 Subsampling하여 feature map의 크기를 14x14로 줄임
c3에서는 5x5 Convolution 연산하여 10x10 사이즈의 16개의 feature map을 생섬함
s4에서는 Subsmapling 하여 feature map의 크기를 5x5로 줄임
c5에서는 5x5 Convolution 연산하여 1x1사이즈의 120개의 feature map을 생성함

Lenet의 구조는 신경망이 깊어질수록 높이와 폭이 줄어드는 경향이 있다.
Lenet은 최근에 softmax(다중분류)를 많이 사용함

Alexnet
Lenet과 유사한 구조를 가지고 있지만, 더크다는게 차이점
ReLU를 사용하며, Local Response Normalization(LRN)을 사용함, 연구를 통해서 성능에 크게 영향을 미치지 않는다고 밝혀짐
Local Response Normalization ReLU는 양수의 방향으로는 입력의 값을 그대로 사용 그렇게 되면 convolution이나 pooling시 매우 높은 하나의 픽셀값이 주변의 픽셀에 영향을 미치게됨
이런 부분을 방지하기 위해 다른 ActivationMap의 같은 위치에 있는 픽셀끼리 정규화를 하는것 

Image Detection

1. Region Proposal - 카테고리와 무관하게 물체의 영역을 찾는 모듈
2. CNN - 각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 Large Convolutional Neural Network
3. SVM - Classification 을 위한 선형 지도학습 모델 Support Vector Machine(SVM) 기준선을 찾아서 그 기준선을 기준으로 나누는 방법
4. RoI Pooling Layer - RoI 영역에 해당하는 부분만 max-pooling을 통해 feature map으로부터 고정된 길이의 저차원 백터로 축소하는 단계를 의미함 RoI = 정해진 비율이 따로 있음 인풋이미지의 크기와 피처맵의 크기가 다를경우 그비율을 구해서 RoI를 조정한다음 진행

R-CNN
알고리즘 순서
1. 이미지를 input에 집어넣음 
2. 2000개의 영역(bounding box)을 selective search 알고리즘을 통해 추출하여 잘라낸다 
2-1. 이를 CNN모델에 넣기 위해 같은 사이즈(227x227 pixel size)로 찌그려뜨린다(Warping).
3. 2000개의 Warped image를 각각 CNN 모델에 집어 넣는다.
4. 각각 Classification을 진행하여 결과를 도출한다.

R-CNN은 Region Proposal 단계에서 seletive search를 사용함 seletive search = 여러가지의 랜덤한 박스를 임의로 생성해놓고 합쳐서 점정 물체를 찾아나가는 방식
그리고 seletive search를 통해서 생성된 2000개의 각각의 이미지를 CNN에 넣는다.
마지막 SVM(Support vector machine)사용 왜냐하면 softmax를 사용했을 경우 mAP값이 54.2%애서 50.9%로 떨어짐
bounding box regression => 임의로 생성된 box를 x, y좌표위치와 너비 높이등을 수정해서 변형하는것을 학습시키는것

단점 
1. 학습이 오래걸린다 => Region Proposal 에서 seletive search 한 이미지 2000개를 전부 CNN에 넣기때문에 오래 걸릴 수 밖에없다.
2. 복잡하다 => CNN, SVM, Bounding box regression 까지 총 세가지 모델을 필요로 하는 매우 복잡한 구조를 가지고있다.
3. Back propagation => 앞에서 말한 SVM과 Bounding box regression에서 학습한 결과가 CNN을 업데이트 하지 못한다.
4.합성곱 신경망(conv)의 입력을 위한 고정된 크기를 위해 wraping/crop을 사용해야 되며, 그 과정에서 input 이미지 정보의 손실이 일어난다.

Fast R-CNN
CNN특징 추출 및 classification, bounding box regression을 모두 하나의 모델에서 학습시키고자 한 모델이다.
알고리즘 순서
1. 전체 이미지를 미리 학습된 CNN을 통과시켜 feature map을 추출한다.
2.feature map에서 Rol(Region of Interest)들을 찾아준다. +)ROI들은 input 이미지에서 Selective Search를 통해 찾은것을 feature map에 적용한 것이다.
3.(Selective Search를 통해 찾은) 각각의 RoI에 대해 RoI Pooling을 진행하여 고정된 크기의 백터를 추출한다.
4.feature vector는 fully vector들을 통과한 후, softmax와 bounding box regression의 input으로 들어간다.
+)softmax는 SVM을 대신하는 방법으로, 해당RoI가 어던 물체인지를 classification한다. +)bounding box regression은 selective search로 찾은 박스의 위치를 조절한다.

단점
R-CNN 과 SPPNet에 비해 빠른 연산 속도와 정확도를 나타낼 수 있었으나 여전히 Region proposal을 selective search로 수행하여 Region proposal연산이 느리다는 단점이 있음

yolo
알고리즘 순서
1. 입력이미지를 S X S 그리드 영역으로 나눠 준다.
2.각각의 grid cell은 B개의 Bounding box와 각 Bounding box에 대한 confidence score를 갖는다(만약 cell에 object가 없다면 confidence score는 0이 된다.)confidence score는 이 시스템이 물체를 포함한다는 예측을 얼마나 확신하는지, 박스에 대하나 예측이 얼마나 정확할지를 의미함
3.각각의 bounding box는 x,y,w,h와 confidence로 구성됨 이 값들을 모두 0~1 범위의 값으로 정규화함
4.각각의 grid cell은 C(conditional class probability)를 갖는다. = class 개수를 받음
5.평가할 때 conditional class probability와 각 박스의 class - specific confidence score를 주는 confidence prediction을 곱했다. 이점수는 class가 박스안에 존재하는지와 박스가 물체에 얼마나 적합한가를 모두 포함한다.
특징
1.이미지 전체를 한번만 보는 것이다. YOLO 이전의 R-CNN은 이미지를 여러장으로 분활하고, CNN모델을 이용해 이미지를 분석함 하지만 YOLO는 이미지 한 번만 보고 실행하기 떄문에 속도가 빠름
2.통합된 모델을 사용함 기존 Object Detection모델은 다양한 전처리 모델과 인공 신경망을 결합해서 사용함 하지만 YOLO는 통합된 모델을 사용해 다른 네트워크 보다 간단함
3.실시간으로 객체를 탐지 할 수 있는것이다. YOLO는 높은성능은 아니더라도 준수한 성능으로 실시간 Object Detection이 가능했기 때문에 기존의 Faster R-CNN보다 6배 빠른 성능을 보여준다.

장점
1. 간단한 처리과정으로 속도가 매우 빠르면 기존은 실시간 object detection 모델들과 비교하면 2배정도 높은 mAP를 보인다.
2. 이미지 전체를 한번에 바라보는 방식을 이용하므로 calss에 대한 맥략적 이해도가 다른 모델에 비해 높아 낮은 False-Positive를 보인다.
3. 일반화된 object학습이 가능하여 자연 이미지로 학습하고 이름 그림과 같은 곳에 테스트 해도 다른 모델에 비해 휠씬 높은 성능을 보여줌

Segmentation은 semantic Segmentation과 Instance Segmentation으로 나뉨
semantic Segmentation은 각각의 물체가 어떤 class인지만 구분함

AlexNet, VGG등 분류에 자주 쓰이는 깊은 신경망들은 Semantic Segmentation을 하는데 적합하지 않음 왜냐하면 parameter의 개수와 차원을 줄이는 layer를 가지고 있어서 위치정보를 잃음
Pooling과 Fully connected layer 를 없애고 stride가 1이고 padding도 일정한 Convolution을 진행해서 하면 가능은하나 parameter의 개수가 많아져서 메모리 문제나 계산하는데 비용이 너무많이듬

FCN(Fully Convolutional Network)
1 x 1 size Convolution layer(Convolutionalization)만 사용한것이 특징
Fully Connect layer를 사용하기 위해서는 고정된 input size를 가질 수 밖에 없다 그리고 FC layer를 지나는 순간 각 pixel에 대해 위치정보는 소실된다. 따라서 FCN은 모든 Network를 Convolution layer만 사용함으로써 input size의 제한을 받지 않고 위치정보를 보전할 수 있게 만듬 
Instance Segmentation은 같은 class일지라도 서로 다른 물체일 경우 구분함
